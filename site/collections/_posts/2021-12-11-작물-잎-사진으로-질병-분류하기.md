---
date: 2021-12-11 00:00:00
title: 작물 잎 사진으로 질병 분류하기
description: 이미지 분류 모델을 활용하여 작물 잎 사진의 종류와 질병 유무를 분류
tags:
  - 다중분류
  - tensorflow
image: /uploads/2021-12-11-20-21-13.png
---
이미지 분류 모델을 활용하여 작물 잎 사진의 종류와 질병 유무를 분류해보자. 데이터는 Kaggle 데이터를 사용했다. 전이학습(Transfer Learning)에 대한 개념을 확립하고 기본 Convolutional Neural Network(CNN) 모델과 학습 방식에서 어떠한 차이점과 장점이 알아보자. 프로젝트에 사용하는 총 데이터 수는 약 40,000개이고, 분류 클래스와 각 클래스의 해당하는 데이터 수는 아래와 같다.

\[표 삽입\]

\| class \| images \| class \| images \| class \| images \|<br>\| — \| — \| — \| — \| — \| — \|<br>\| Apple\\\_scab \| 630 \| Corn \| 1,162 \| Septoria\\\_leaf \| 1,771 \|

높은 성능의 이미지 분류 모델을 구축하기 위해서는 **많은 수**의&nbsp; **질 좋은**&nbsp;데이터셋이 필요하다. 하지만 실제로 양질의 데이터셋을 대량으로 구하기는 매우 어렵다. 실제로 회사에서 프로젝트를 들어가게 되면 느낄 수 있다. 그렇다면 이 문제를 해결할 수 있는 방법은 무엇이 있을까? 이런 방법들을 딥러닝 개발자들이 해결했고, 우리는 그 것을 이해하고 사용할 수 있으면 된다.&nbsp;

1. 대량의 데이터셋으로 미리 학습된 모델을 재활용한다.
2. 일부를 조정하여 다른 주제의 이미지 분류 모델에 사용한다.

이 때, 대량의 데이터셋으로 미리 학습된 모델은 **Pre-Trained Model**이며, **Pre-Trained Model**을 조정하는 과정을 **Fine-Tuning**이라 한다. 그리고 이 과정을 통틀어서 **Transfer Learning**이라 한다.

> ## Pre-Trained Model

​일반적으로 Pre-Trained Model은 **ImageNet**의 훈련 데이터를 이용하며, 약 1,400만개의 이미지를 1,000개의 클래스로 분류하는 학습을 한다.&nbsp;

![](/uploads/2021-12-11-20-30-39.png){: width="1414" height="540"}

\[그림1\] 트랜스퍼러닝

![](/uploads/2.png){: width="2354" height="1070"}

\[그림2\] 데이터 크기 - 유사성 그래프

이미지 분류 모델은 먼저 낮은 수준의 특징(Low-Level Features)을 학습하고, 그 것을 토대로 더 &nbsp;높은 수준의 특정(High-Level Features)을 학습한다. Low-Level Features는 모서리와 같이 작은 지역 패턴을 의미하고, High-Level Features는 Low-Level Features로 구성된 더 큰 패턴을 의미한다. 다른 종류의 이미지라도 낮은 수준의 특징은 비슷할 가능성이 높다. 이 때문에 낮은 수준의 특징은 Freeze한다. 이 때, Freeze하는 Layer의 수는 데이터셋의 크기와 Pre-trained model에 사용된 데이터셋과 유사성을 고려하여 결정한다.&nbsp;

\[그림 2\]은 우리가 가진 데이터셋의 크기와 우리의 데이터셋이 Pre-Trained Model의 학습에 사용된 데이터와 얼마나 유사한지에 따라 4가지 경우로 구분하고 있다. 모델은 이미지의 Feature를 학습하는 Convolutional Base 부분과 이미지를 분류하는 Classifier 부분으로 나뉠 수 있는데 분류하는 이미지가 다를 경우 Classifier 부분을 변경해주어야 한다.

1사분면의 경우, **우리가 가진 데이터의 수도 많고, 원래의 학습에 이용된 데이터와의 유사도도 높다.** 이 경우에는 그림에서와 같이 일부 Layer만을 Feeze하는 것이 좋다. 데이터의 유사도가 높기 때문에 상대적으로 적은 수의 Layer만 업데이트해도 높은 성능을 낼 수 있다.&nbsp;

2사분면의 경우, **우리가 가진 데이터의 수는 많지만, Pre-Trained Model의 학습에 사용된 데이터와는 유사도가 낮다.**&nbsp;이 경우 \[그림 2\]에서와 같이 Pre-Trained Model을 불러온 후, 전체 Layer를 재학습하는 것이 좋다. 모든 Layer가 학습과정에서 업데이트 되지만, 모델의 효과적인 구조는 그대로 유지되고, 불러온 Parameter 값에서 조금씩 업데이트하며 조정한다는 점이 처음부터 모델을 구성하는 방법과의 차이점이 있다.

3사분면의 경우, **우리가 가진 데이터의 수는 적지만 원래의 학습에 이용된 데이터와의 유사도가 높다.** 이 경우에는 데이터의 유사도가 높으므로 &nbsp;Pre-Trained Model이 학습한 많은 Feature들을 그대로 사용할 수 있지만, 일부 Layer를 재학습하기에는 과적합의 위험이 상대적으로 크다. 따라서 \[그림 2\]에서와 같이 모델의 Convolutional Base 부분 전체를 Freeze하고, Classifier 부분만 변경하는 것이 효과적이다.

4사분면의 경우, **우리가 가진 데이터의 수는 적지만 원래의 학습에 이용된 데이터와의 유사도가 높다.** 이 경우에는 데이터의 유사도가 높으므로 Pre-Trained Model이 학습한 많은 Feature들을 그대로 사용할 수 있지만, 일부 Layer를 재학습하기에는 과적합의 위험이 상대적으로 크다. 따라서 \[그림 2\]에서와 같이 모델의 Convolutional Base부분 전체를 Freeze하고, Classifier 부분만 변경하는 것이 효과적일 수 있다.

이프로젝트에서는 ResNet50 모델을 불러온 후, Fine-Tuning하여 작물 잎을 분류하는 주제에 맞추어 사용한다.

소스코드:&nbsp;[https://github.com/woohaen88/leaf-classification](https://github.com/woohaen88/leaf-classification){: target="_blank" rel="noopener"}

​
